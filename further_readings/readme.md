# Further Readings

Here we list further (or related, preliminary) readings for the course. Due to our limited time to prepare for the course, we must have missed many excellent resources. Please help us improve and expand the list by creating pull requests!
If we find it useful, we will actively merge your pull request (and of course, thus you will be listed as one of the contributors to the course).

## A. Prerequistes


## B. Related Topics


## C. Corresponding to Each Lecture
#### L1
1. [【视频】CS11-747 CMU自然语言处理课程](https://www.bilibili.com/video/BV1Sb4y1X7pY?spm_id_from=333.337.search-card.all.click&vd_source=3483666ddd2c29d930f2a3236d715dee)
2. [【视频】CS224n 斯坦福自然语言处理课程](https://web.stanford.edu/class/cs224n/)
3. [【视频】大模型概况简介](https://www.youtube.com/watch?v=UB_p3s5rBSM)
4. [【网页】十个自然语言处理大模型](https://www.topbots.com/leading-nlp-language-models-2020/)
5. [【论文】Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)
6. [【论文】基础模型综述(李飞飞等)](https://arxiv.org/abs/2108.07258)

#### L2
1. [【视频】CS11-747 CMU自然语言处理课程](https://www.bilibili.com/video/BV1Sb4y1X7pY?spm_id_from=333.337.search-card.all.click&vd_source=3483666ddd2c29d930f2a3236d715dee)
2. [【视频】CS224n 斯坦福自然语言处理课程](https://web.stanford.edu/class/cs224n/)
3. [【网页】pytorch官方教程](https://pytorch.org/tutorials/)

#### L3
1. [【论文集】预训练模型必读论文](https://github.com/thunlp/PLMpapers)
2. [Transformer原论文: Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

#### L4
1. [【论文集】Prompt Tuning论文列表](https://github.com/thunlp/PromptPapers)
2. [【论文集】Delta Tuning论文列表](https://github.com/thunlp/DeltaPapers)
3. [【工具包】OpenPrompt](https://github.com/thunlp/OpenPrompt)
4. [【工具包】OpenDelta](https://github.com/thunlp/OpenDelta)

#### L5
1. [【论文】BMInf: An Efficient Toolkit for Big Model Inference and Tuning](https://aclanthology.org/2022.acl-demo.22/)
2. [【网页】BMTrain：为大模型训练计算成本节省9成](https://www.openbmb.org/blogs/blogpage?id=8c130256a3d14b8ca88c59a212da2e38)
3. [【网页】不止于ZeRO：BMTrain技术原理浅析](https://www.openbmb.org/blogs/blogpage?id=3bff5bc60f96408f81101774c2aec7dd)
4. [【论文】ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)
5. [【论文】TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)
6. [【论文】Structured Pruning Learns Compact and Accurate Models](https://arxiv.org/abs/2204.00408)
7. [【论文】MoEfication: Transformer Feed-forward Layers are Mixtures of Experts](https://aclanthology.org/2022.findings-acl.71.pdf)

#### L6
1. 信息检索
    1. [【论文】Dense Passage Retrieval for Open-Domain Question Answering](https://aclanthology.org/2020.emnlp-main.550/)
    2. [【论文】Document Ranking with a Pretrained Sequence-to-Sequence Model](https://aclanthology.org/2020.findings-emnlp.63/)
    3. [【论文】BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models ](https://openreview.net/forum?id=wCu6T5xFjeJ)

2. 阅读理解/问答
    1. [【论文】SQuAD: 100,000+ Questions for Machine Comprehension of Text](https://www.aclweb.org/anthology/D16-1264.pdf)
    2. [【论文】Reading Wikipedia to Answer Open-Domain Questions](https://www.aclweb.org/anthology/P17-1171.pdf)
    3. [【论文】UNIFIEDQA: Crossing Format Boundaries with a Single QA System](https://aclanthology.org/2020.findings-emnlp.171/)
    4. [【网页】WebGPT: Browser-assisted question-answering with human feedback ](https://openai.com/blog/webgpt/)

3. 文本生成
    1. [【论文】Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation](https://arxiv.org/abs/1703.09902)
    2. [【论文】Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension](https://arxiv.org/abs/1910.13461)
    3. [【论文】Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190)

#### L7
1. [【论文】AlphaFold2: Highly accurate protein structure prediction with AlphaFold](https://www.nature.com/articles/s41586-021-03819-2)
2. [【论文】Enformer: Effective gene expression prediction from sequence by integrating long-range interactions](https://www.nature.com/articles/s41592-021-01252-x)
3. [【论文】DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome](https://academic.oup.com/bioinformatics/article-abstract/37/15/2112/6128680)
4. [【论文】biomedical NLP综述: Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing.](https://dl.acm.org/doi/abs/10.1145/3458754?casa_token=Pnh83RKffIEAAAAA:WWAZcNeaOUZSVeRMpuLKEgH4QY3GzXw0DD9Vt_ZXt-B2tUs02m9p-T2VABvmVDQA1M12hcS6bNM_)
5. [【工具包】TorchDrug](https://github.com/DeepGraphLearning/torchdrug)

#### L8
1. [【论文】How does NLP benefit legal system: A summary of legal artificial intelligence](https://arxiv.org/abs/2004.12158)
2. [【论文】Lawformer: A pre-trained language model for chinese legal long documents](https://www.sciencedirect.com/science/article/pii/S2666651021000176)
3. [【论文】LeCaRD: a legal case retrieval dataset for Chinese law system](https://dl.acm.org/doi/pdf/10.1145/3404835.3463250?casa_token=10TOn3fFSC0AAAAA:P8kTbHoP8FlLQO5cej5enTEG_MI9sw4urliU9zPvw7Z7wH9dNT7UpLXCJUKOhzpEwXm-cnQvVYhC)
4. [【论文】LEVEN: A Large-Scale Chinese Legal Event Detection Dataset](https://arxiv.org/abs/2203.08556)
5. [【论文】LEGAL-BERT: The muppets straight out of law school ](https://arxiv.org/pdf/2010.02559)

#### L9
1. [【论文】MoEfication: Transformer Feed-forward Layers are Mixtures of Experts](https://aclanthology.org/2022.findings-acl.71.pdf)
2. [【论文】Finding Experts in Transformer Models ](https://aclanthology.org/2022.findings-acl.71.pdf)
3. [【论文】Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain)](https://arxiv.org/abs/1905.11833v4)
4. [【论文】Exploring Universal Intrinsic Task Subspace via Prompt Tuning](https://arxiv.org/pdf/2110.07867.pdf)
5. [【论文】On Transferability of Prompt Tuning for Natural Language Understanding](https://arxiv.org/abs/2111.06719)